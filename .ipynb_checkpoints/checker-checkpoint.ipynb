{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import requests\n",
    "import json\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(original):\n",
    "    text = original\n",
    "    text = urllib.parse.quote(text)\n",
    "    text = '\"' + text + '\"'\n",
    "    url = 'https://google.com/search?q={}'.format(text)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    for g in soup.find_all(class_='ZINbbc xpd O9g5cc uUPGi'):\n",
    "        here = (original in g.text and \"did not match any documents\" not in g.text)\n",
    "        if here:\n",
    "            s = original + \"\\n\" + \"----------------------------\" + \"\\n\" + g.text\n",
    "            return(s)\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_txt(pdf_filepath, password='', max_pages=0, caching=True, page_num=set()):\n",
    "     \n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    laparams = LAParams()\n",
    "\n",
    "    # Change codec if needed.\n",
    "    codec = 'utf-8'  # 'utf16','utf-8'\n",
    "    with StringIO() as return_string, open(pdf_filepath, 'rb') as open_file:\n",
    "        with TextConverter(rsrcmgr, return_string, codec=codec, laparams=laparams) as device:\n",
    "            interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "            for page in PDFPage.get_pages(open_file, page_num, maxpages=max_pages, password=password, caching=caching, check_extractable=True):\n",
    "                interpreter.process_page(page)\n",
    "            paper_string = return_string.getvalue()\n",
    "            # print(paper_string)\n",
    "            return paper_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_quotes(text):\n",
    "    inquote = False\n",
    "    quotes = []\n",
    "    index = 0\n",
    "    for item in text.split():\n",
    "        curr = len(quotes)-1\n",
    "        if(len(item.strip()) > 0 and item):\n",
    "            try:\n",
    "                if(inquote):\n",
    "                        quotes[curr] = quotes[curr] + \" \" + item\n",
    "                elif(item[0] == \"\\\"\" or item[0] == '“'): \n",
    "                    inquote = True\n",
    "                    quotes.append(item)\n",
    "                if(item[len(item)-1] == \"\\\"\" or item[len(item)-1] == '”' or item[len(item)-2] == \"\\\"\" or item[len(item)-2] == '”'):\n",
    "                            inquote = False\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "                print(item)\n",
    "                print(index)\n",
    "                print(quotes)\n",
    "                print(len(quotes))\n",
    "                break\n",
    "    for index in range(len(quotes)):\n",
    "        curr = quotes[index]\n",
    "        quotes[index] = curr[1:len(curr)-1]\n",
    "        quotes[index] = quotes[index].replace(u'’', u\"'\")\n",
    "        quotes[index] = quotes[index].replace(u'”', u\"\")\n",
    "        quotes[index] = quotes[index].replace('- ', '')\n",
    "        \n",
    "    returnlist = [x for x in quotes if (len(x.split()) > 1)]\n",
    "\n",
    "    return(returnlist)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_quotes(quotes):\n",
    "    for quote in quotes:\n",
    "        test = check(quote)\n",
    "        if test:\n",
    "            print(test)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory = r'./PDFs'\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        print(filename)\n",
    "        print()\n",
    "        check_quotes(find_quotes(convert_pdf_to_txt('./PDFs/' + filename)))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rich emotional, psychological and sociocultural phenomenon', 'people are more likely to swear in relaxed environments than in formal environments', 'you are so gay', 'Bill Clinton urges Illinois to approve gay marriage bill', 'the main purpose of cursing is to express emotions, especially anger and frustration.', 'And all I need is one fuckin sheet stamped! #rage', 'fucking love you.', 'Where da fuq is the sun at, this weather is so #depressing', 'My life fell apart a long ass time ago.. So everythings normal i guess.', 'Soo pissed off', \"People laugh when I say I work at McDonald's. And I say, bitch at least I have a job! At least I don't bother my parents asking them for $$$\", 'Across the ocean, across the sea starting to hate the fucking distance between Justin Bieber and me.', '@user you little whore TAKE ME WITH YOU', 'Dear Marilyn Manson, I fucking love you and your music. The end.', 'profane al. comments are more popular or more widely read than nonprofane comments', 'who says curse words to whom', '@Harry Styles follow me babe<3', \"@NiallOfﬁcial I can't sleep :(\", '@taylorswift13 slut', '@Harry Styles slut drop on my follow button :))))))))']\n"
     ]
    }
   ],
   "source": [
    "print(find_quotes(convert_pdf_to_txt('./Twitter_CSCW/' + '2531602.2531734.pdf')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rich emotional, psychological and sociocultural phenomenon\n",
      "----------------------------\n",
      "Scholarly articles for \"rich emotional, psychological and sociocultural phenomenon\"scholar.google.com › citationsI may talk in English but gaali toh Hindi mein hi denge: …Agarwal • Cited by 3The utility and ubiquity of taboo wordsJay • Cited by 376Use and perception of taboo language in college-age …Uhlman • Cited by 4\n",
      "\n",
      "people are more likely to swear in relaxed environments than in formal environments\n",
      "----------------------------\n",
      "The relevance of taboo language: An analysis of the indexical ...https://www.researchgate.net › publication › 259128466_The_relevance_o...... have the powerful influence on the likelihood of swearing and people are more likely to swear in relaxed environments than in formal environments (Johnson, ...\n",
      "\n",
      "you are so gay\n",
      "----------------------------\n",
      "you are so gay love will on Spotifyhttps://open.spotify.com › playlistyou are so gay love will. By Amie Thompson. 40 songs. Play on Spotify. 1. Gold DiggerKanye West, Jamie Foxx • Late Registration. 3:270:30. 2. Hard Knock Life  ...\n",
      "\n",
      "Bill Clinton urges Illinois to approve gay marriage bill\n",
      "----------------------------\n",
      "in “Bill Clinton urges Illinois to approve gay marriage bill” does not convey cursing . To achieve high precision in iden- tifying cursing expressions, we keep only the words that are mostly associated with cursing connotation.Cursing in English on twitter - ACM Digital Libraryhttps://dl.acm.org › doi › pdf\n",
      "\n",
      "the main purpose of cursing is to express emotions, especially anger and frustration.\n",
      "----------------------------\n",
      "The Relationship Between Profanity and Honesty - Gilad Feldman ...https://mafiadoc.com › ...Jay and Janschewitz (2008) summarized that “the main purpose of cursing is to express emotions, especially anger and frustration.” (p. 267). Therefore, if we ...\n",
      "\n",
      "Soo pissed off\n",
      "----------------------------\n",
      "RM 227 Kwang Soo pissed off at Ryu Hyun Jin - YouTubehttps://www.youtube.com › watchOct 8, 2015 · RM 227 Kwang Soo pissed off at Ryu Hyun Jin. yanderezaf. Loading... Unsubscribe from yanderezaf? Cancel Unsubscribe. Working.\n",
      "\n",
      "People laugh when I say I work at McDonald's. And I say, bitch at least I have a job! At least I don't bother my parents asking them for $$$\n",
      "----------------------------\n",
      "Vloeken op twitter | Profanity | Digital & Social Media - Scribdhttps://www.scribd.com › document › Vloeken-op-twitterFeb 22, 2014 · ... “People laugh when I say I work at McDonald's. And I say, bitch at least I have a job! At least I don't bother my parents asking them for $$$”.\n",
      "\n",
      "Across the ocean, across the sea starting to hate the fucking distance between Justin Bieber and me.\n",
      "----------------------------\n",
      "Vloeken op twitter | Profanity | Digital & Social Media - Scribdhttps://www.scribd.com › document › Vloeken-op-twitterFeb 22, 2014 · “Across the ocean, across the sea starting to hate the fucking distance between Justin Bieber and me.” “@user you little whore TAKE ME WITH ...\n",
      "\n",
      "@user you little whore TAKE ME WITH YOU\n",
      "----------------------------\n",
      "Cursing in English on twitter - ACM Digital Libraryhttps://dl.acm.org › doi › pdf“@user you little whore TAKE ME WITH YOU”. “Dear Marilyn Manson, I fucking love you and your music. The end.” Table 4. Example tweets in which curse ...\n",
      "\n",
      "Dear Marilyn Manson, I fucking love you and your music. The end.\n",
      "----------------------------\n",
      "Vloeken op twitter | Profanity | Digital & Social Media - Scribdhttps://www.scribd.com › document › Vloeken-op-twitterFeb 22, 2014 · ... Justin Bieber and me.” “@user you little whore TAKE ME WITH YOU” “Dear Marilyn Manson, I fucking love you and your music. The end.” ...\n",
      "\n",
      "who says curse words to whom\n",
      "----------------------------\n",
      "Studies Show That the F-Word is the Most Popular Curse Word ...https://www.complex.com › 2014/02 › fuck-is-most-popular-on-twitterFeb 21, 2014 · The study breaks down four major research topics: ubiquity, utility, contextual variables, and who says curse words to whom on Twitter. Ubiquity ...\n",
      "\n",
      "@Harry Styles follow me babe<3\n",
      "----------------------------\n",
      "[PDF] Cursing in English on Twitter - CORE Scholar - Wright State Universityhttps://corescholar.libraries.wright.edu › cgi › viewcontentfans, e.g., “@Harry Styles follow me babe<3”, “@NiallOffi- cial I can't sleep :(”. Besides the cursing ratios in tweets that are posted/received by different user ...\n",
      "\n",
      "@taylorswift13 slut\n",
      "----------------------------\n",
      "[PDF] Automatic Emotion Identification from Text - CORE Scholar - Wright ...https://corescholar.libraries.wright.edu › cgi › viewcontentSep 4, 2015 · their gender for fun, e.g., “@taylorswift13 slut”, “@Harry Styles slut drop on my follow button :))))))))”. 6.3 Limitations. This study is limited in ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_quotes(find_quotes(convert_pdf_to_txt('./Twitter_CSCW/' + '2531602.2531734.pdf')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_twitter(original):\n",
    "    text = original\n",
    "    text = urllib.parse.quote(text)\n",
    "    url = 'https://twitter.com/search?q={}&src=recent_search_click'.format(text)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    file = open('test.html', 'w', encoding='utf-8')\n",
    "    file.write(str(soup))\n",
    "    file.close()\n",
    "    for g in soup.find_all(class_='css-1dbjc4n r-my5ep6 r-qklmqi r-1adg3ll'):\n",
    "        print(g.text)\n",
    "#         here = (original in g.text and \"did not match any documents\" not in g.text)\n",
    "#         if here:\n",
    "#            s = original + \"\\n\" + \"----------------------------\" + \"\\n\" + g.text\n",
    "#             return(s)\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_twitter(\"fdafdsafdsa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting TwitterSearch\n",
      "  Downloading TwitterSearch-1.0.2.tar.gz (20 kB)\n",
      "Requirement already satisfied: requests>=1.0.0 in c:\\users\\espen\\miniconda3\\lib\\site-packages (from TwitterSearch) (2.22.0)\n",
      "Collecting requests-oauthlib>=0.3.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\espen\\miniconda3\\lib\\site-packages (from requests>=1.0.0->TwitterSearch) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\espen\\miniconda3\\lib\\site-packages (from requests>=1.0.0->TwitterSearch) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\espen\\miniconda3\\lib\\site-packages (from requests>=1.0.0->TwitterSearch) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\espen\\miniconda3\\lib\\site-packages (from requests>=1.0.0->TwitterSearch) (2.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Building wheels for collected packages: TwitterSearch\n",
      "  Building wheel for TwitterSearch (setup.py): started\n",
      "  Building wheel for TwitterSearch (setup.py): finished with status 'done'\n",
      "  Created wheel for TwitterSearch: filename=TwitterSearch-1.0.2-py3-none-any.whl size=18443 sha256=1373b002495fd997d0f2637132936c3a6c4185368022dca20f60e4a62f9b3ec0\n",
      "  Stored in directory: c:\\users\\espen\\appdata\\local\\pip\\cache\\wheels\\d5\\74\\5f\\f344ffff31a93ee504581a51aad34120bf05bd749493365840\n",
      "Successfully built TwitterSearch\n",
      "Installing collected packages: oauthlib, requests-oauthlib, TwitterSearch\n",
      "Successfully installed TwitterSearch-1.0.2 oauthlib-3.1.0 requests-oauthlib-1.3.0\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install TwitterSearch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
